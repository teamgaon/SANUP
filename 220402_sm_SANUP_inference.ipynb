{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/SANUP/blob/main/220402_sm_SANUP_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리, 패키지"
      ],
      "metadata": {
        "id": "L4lTVQHqCbcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee9BbI2znxA0",
        "outputId": "2625a514-d37c-46e6-dc3b-798a6f08ab5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31KhLlUBlz_2",
        "outputId": "3cf22b7e-71d4-4e54-a642-46175ce08d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 82.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 87.8 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 92.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 86.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 90.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 92.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 56.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Installing collected packages: urllib3, multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TPU 사용을 위한 accelerate"
      ],
      "metadata": {
        "id": "M0FSjXgjH5ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "_vxmupBjnvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModel,AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
        "import gc\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from accelerate import notebook_launcher"
      ],
      "metadata": {
        "id": "R7yZowtOokD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/SANUP/1. 실습용자료.txt', sep='|', encoding='cp949')\n",
        "test = pd.read_csv('/content/drive/MyDrive/SANUP/2. 모델개발용자료.txt', sep='|', encoding='cp949')\n",
        "submission = pd.read_csv('/content/drive/MyDrive/SANUP/답안 작성용 파일.csv', encoding='cp949')"
      ],
      "metadata": {
        "id": "PpIow7U5n0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "59T4TAjfgro3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna('')"
      ],
      "metadata": {
        "id": "3jRzUsQN_PwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text_obj'].astype(str) + \" \" + df['text_mthd'].astype(str) + \" \" + df['text_deal'].astype(str)\n",
        "df['target'] = df['digit_1'] + \" \" + df['digit_2'].astype(str) + \" \" + df['digit_3'].astype(str)\n",
        "df"
      ],
      "metadata": {
        "id": "kkUI2TfI7Hx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df[['text', 'target']]\n",
        "train"
      ],
      "metadata": {
        "id": "SPWWR9aS7jk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크나이저, 함수"
      ],
      "metadata": {
        "id": "fx9yGQxtClQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGWbl8edlz_5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "\n",
        "checkpoint = \"monologg/koelectra-base-v3-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['labels'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "J3OrqrY3or6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {}\n",
        "for i in range(len(train['target'].unique())):\n",
        "  my_dict[train['target'].unique()[i]] = i\n",
        "\n",
        "def target_to_num(target:str):\n",
        "  return my_dict[target]\n",
        "\n",
        "train['target'] = train['target'].map(target_to_num)\n",
        "train"
      ],
      "metadata": {
        "id": "0rCBWIGn84fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict['0.0 0.0 0.0'] = len(my_dict)"
      ],
      "metadata": {
        "id": "0IlZxhR8iBJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['text'].isnull()]"
      ],
      "metadata": {
        "id": "egKkpYt7-9U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "bs0VFrWuA_6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습\n",
        "\n",
        "* Model\n",
        "  + Roberta-large\n",
        "\n",
        "* Optimizer\n",
        "  + AdamW\n",
        "\n",
        "* Learning rate scheduler\n",
        "  + Cosine annealing with warmup"
      ],
      "metadata": {
        "id": "uep3CPkqCWWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_function():\n",
        "  accelerator = Accelerator()\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, \n",
        "                    batch_size=32, sampler=train_subsampler)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset,\n",
        "                    batch_size=32, sampler=test_subsampler)\n",
        "\n",
        "  config = AutoConfig.from_pretrained(checkpoint)\n",
        "  config.num_labels = len(train['target'].unique())\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "  model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "  num_epochs = 20\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=1,\n",
        "      num_training_steps=num_training_steps,\n",
        "  )\n",
        "  my_acc = []\n",
        "  for epoch in range(num_epochs):\n",
        "      train_acc = 0.0\n",
        "      test_acc = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for batch_id, batch in enumerate(train_dataloader):\n",
        "          outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "          loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "          accelerator.backward(loss)\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "          train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "      print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "      model.eval()\n",
        "      for batch_id, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "        test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "      print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "      my_acc.append(test_acc / (batch_id+1))\n",
        "      gc.collect()\n",
        "  accelerator.wait_for_everyone()\n",
        "  unwrapped_model = accelerator.unwrap_model(model)\n",
        "  unwrapped_model.save_pretrained('/content/drive/MyDrive/220402_inference/model' + str(fold), save_function=accelerator.save)\n",
        "  plt.plot(len(my_acc), my_acc)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "miLIkS4ZQnxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "print('--------------------------------')\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "  list(train['text']),\n",
        "  return_tensors=\"pt\",\n",
        "  max_length=64, # Max_Length = 64\n",
        "  padding=True,\n",
        "  truncation=True,\n",
        "  add_special_tokens=True\n",
        "  )\n",
        "  \n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train, train['target'])):\n",
        "  print(f'FOLD {fold}')\n",
        "\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "  train_label = train['target']\n",
        "\n",
        "  train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "\n",
        "  notebook_launcher(training_function)"
      ],
      "metadata": {
        "id": "vJ937XNoPln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추론"
      ],
      "metadata": {
        "id": "epX2UHDndc-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.fillna('')"
      ],
      "metadata": {
        "id": "-O6E7GjQhTr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['text'] = test['text_obj'].astype(str) + \" \" + test['text_mthd'].astype(str) + \" \" + test['text_deal'].astype(str)\n",
        "test['target'] = test['digit_1'].astype(str) + \" \" + test['digit_2'].astype(str) + \" \" + test['digit_3'].astype(str)\n",
        "test"
      ],
      "metadata": {
        "id": "piixJ6Ui3d4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test[['text', 'target']]\n",
        "test"
      ],
      "metadata": {
        "id": "vhqkdRzayCb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "test_label = test['target'].map(target_to_num)\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    list(test['text']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=64,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "test_dataset = BERTDataset(tokenized_test, test_label)\n",
        "\n",
        "dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "for fold in range(5):\n",
        "\n",
        "  config = AutoConfig.from_pretrained(checkpoint)\n",
        "  config.num_labels = 225\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/220402_inference/model' + str(fold), num_labels=225)\n",
        "  model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "  accelerator = Accelerator()\n",
        "  model = accelerator.unwrap_model(model)\n",
        "\n",
        "  output_pred = []\n",
        "  output_prob = []\n",
        "\n",
        "  model, dataloader= accelerator.prepare(model, dataloader)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for i, data in enumerate(tqdm(dataloader)):\n",
        "      with torch.no_grad():\n",
        "          outputs = model(\n",
        "              input_ids=data['input_ids'],\n",
        "              attention_mask=data['attention_mask']\n",
        "          )\n",
        "      logits = outputs[0]\n",
        "      prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      result = np.argmax(logits, axis=-1)\n",
        "      output_pred.append(result)\n",
        "      output_prob.append(prob)\n",
        "      \n",
        "  pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
        "\n",
        "  def num_to_label(label):\n",
        "      label_dict = {v: k for k, v in my_dict.items()}\n",
        "      str_label = []\n",
        "\n",
        "      for i, v in enumerate(label):\n",
        "          str_label.append([i,label_dict[v]])\n",
        "      \n",
        "      return str_label\n",
        "\n",
        "  answer = num_to_label(pred_answer)\n",
        "\n",
        "  df_label = pd.DataFrame(answer, columns=['index', 'label'])\n",
        "  df_prob = pd.DataFrame(output_prob)\n",
        "\n",
        "  df_label.to_csv('/content/drive/MyDrive/220402_inference/pred_label'+str(fold)+'.csv', index=False)\n",
        "  df_prob.to_csv('/content/drive/MyDrive/220402_inference/pred_prob'+str(fold)+'.csv', index=False)"
      ],
      "metadata": {
        "id": "kQesDl_0qsuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "id": "ybhmYNaOfCX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "m94JxCJRuijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred0 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_label0.csv')\n",
        "pred1 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_label1.csv')\n",
        "pred2 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_label2.csv')\n",
        "pred3 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_label3.csv')\n",
        "pred4 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_label4.csv')"
      ],
      "metadata": {
        "id": "npXR96pIBZl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['label0'] = pred0['label']\n",
        "test['label1'] = pred1['label']\n",
        "test['label2'] = pred2['label']\n",
        "test['label3'] = pred3['label']\n",
        "test['label4'] = pred4['label']\n",
        "df = test"
      ],
      "metadata": {
        "id": "ijSuvL_B6Hdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "3gYfQp2k6I8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1-((len(df)-len(df[(df['label0'] == df['label1']) & (df['label1'] == df['label2']) & (df['label2'] == df['label3']) & (df['label3'] == df['label4'])]))/len(df))"
      ],
      "metadata": {
        "id": "6md6HQBPv7Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hard = df[df.merge(df[(df['label0'] == df['label1']) & (df['label1'] == df['label2']) & (df['label2'] == df['label3']) & (df['label3'] == df['label4'])], how = 'left', left_index=True, right_index=True)['text_y'].isnull()]\n",
        "hard"
      ],
      "metadata": {
        "id": "BD6QoKnmzYWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred0 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_prob0.csv')\n",
        "pred1 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_prob1.csv')\n",
        "pred2 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_prob2.csv')\n",
        "pred3 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_prob3.csv')\n",
        "pred4 = pd.read_csv('/content/drive/MyDrive/220331_inference/pred_prob4.csv')"
      ],
      "metadata": {
        "id": "7D976Pqds77c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pd.DataFrame((np.array(pred0) + np.array(pred1) + np.array(pred2) + np.array(pred3) + np.array(pred4))/5)\n",
        "# test = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
        "test = pd.concat([test, pred], axis=1)\n",
        "test.head(3)"
      ],
      "metadata": {
        "id": "m4MyzZvxmlJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_label(label):\n",
        "    label_dict = label_dict = {v: k for k, v in my_dict.items()}\n",
        "    str_label = []\n",
        "\n",
        "    for i, v in enumerate(label):\n",
        "        str_label.append([i,label_dict[v]])\n",
        "    \n",
        "    return str_label\n",
        "\n",
        "answer = num_to_label(np.argmax(np.array(pred), axis=-1))\n",
        "\n",
        "test['soft'] = pd.DataFrame(answer)[1]\n",
        "test.head(3)"
      ],
      "metadata": {
        "id": "-yPmoNF50EL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft = test[['label0', 'label1', 'label2', 'label3', 'label4', 'soft']]\n",
        "soft"
      ],
      "metadata": {
        "id": "AasCZGaqT5EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_count(df):\n",
        "  num_count = {}\n",
        "  cols = ['label0', 'label1', 'label2', 'label3', 'label4']\n",
        "\n",
        "  for col in cols:\n",
        "    try: num_count[df[col]] += 1\n",
        "    except: num_count[df[col]] = 1\n",
        "  return(num_count)\n",
        "\n",
        "temp = soft.apply(label_count, axis=1)\n",
        "soft['temp'] = temp\n",
        "\n",
        "def label_count(temp:dict):\n",
        "  keys = []\n",
        "  temptemp = 0\n",
        "  for k, v in temp.items():\n",
        "    if v >= 3:\n",
        "      temptemp = k\n",
        "  keys.append(temptemp)\n",
        "  return keys[0]\n",
        "  \n",
        "soft['hard'] = soft['temp'].map(label_count)"
      ],
      "metadata": {
        "id": "RmjBGd23WtuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft[soft['soft'] != soft['hard']]"
      ],
      "metadata": {
        "id": "XdBHlTkJW4CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = soft[['label0',\t'label1',\t'label2',\t'label3',\t'label4',\t'soft',\t'hard']]\n",
        "df"
      ],
      "metadata": {
        "id": "KVYfl6gzeOGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "EHYIEXZIon1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "id": "K4c6Zg0z4DXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_label(text:str):\n",
        "  return text.split(' ')[0]\n",
        "\n",
        "submission['digit_1'] = df['soft'].map(split_label)\n",
        "\n",
        "def split_label(text:str):\n",
        "  return text.split(' ')[1]\n",
        "\n",
        "submission['digit_2'] = df['soft'].map(split_label)\n",
        "\n",
        "def split_label(text:str):\n",
        "  return text.split(' ')[2]\n",
        "\n",
        "submission['digit_3'] = df['soft'].map(split_label)"
      ],
      "metadata": {
        "id": "Pezga-4peM06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "Jst5B_vZpBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/220331_inference/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "P99C7cKApD6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "220315_sm_SANUP_TPU.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}