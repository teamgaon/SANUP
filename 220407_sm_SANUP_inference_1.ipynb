{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/SANUP/blob/main/220407_sm_SANUP_inference_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리, 패키지"
      ],
      "metadata": {
        "id": "L4lTVQHqCbcw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31KhLlUBlz_2",
        "outputId": "053cd788-f508-4322-d543-3fd823521639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 83.0 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.3 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 77.6 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 83.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 72.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 84.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 80.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Installing collected packages: urllib3, multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.6.2-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.10.0+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.6.2\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 36 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.4 kB/s \n",
            "\u001b[?25hCollecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.10.0.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.56.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: google-api-python-client, torch-xla, torch, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.11\n",
            "    Uninstalling google-api-python-client-1.12.11:\n",
            "      Successfully uninstalled google-api-python-client-1.12.11\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers\n",
        "# !pip install git+https://github.com/ssut/py-hanspell.git\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ee9BbI2znxA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TPU 사용을 위한 accelerate"
      ],
      "metadata": {
        "id": "M0FSjXgjH5ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "_vxmupBjnvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModel,AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
        "import gc\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from accelerate import notebook_launcher\n",
        "from pathlib import Path\n",
        "# from hanspell import spell_checker\n",
        "from pykospacing import Spacing"
      ],
      "metadata": {
        "id": "R7yZowtOokD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/SANUP/1. 실습용자료.txt', sep='|', encoding='cp949')\n",
        "test = pd.read_csv('/content/drive/MyDrive/SANUP/2. 모델개발용자료.txt', sep='|', encoding='cp949')\n",
        "submission = pd.read_csv('/content/drive/MyDrive/SANUP/답안 작성용 파일.csv', encoding='cp949')"
      ],
      "metadata": {
        "id": "PpIow7U5n0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna('')\n",
        "df['text'] = df['text_obj'].astype(str) + \" \" + df['text_mthd'].astype(str) + \" \" + df['text_deal'].astype(str)\n",
        "df"
      ],
      "metadata": {
        "id": "kkUI2TfI7Hx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크나이저, 함수"
      ],
      "metadata": {
        "id": "fx9yGQxtClQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_something(text:str):\n",
        "  return text.replace('&',' ')\n",
        "\n",
        "df['text'] = df['text'].map(remove_something)\n",
        "\n",
        "def remove_something(text:str):\n",
        "  return text.replace('.',' ')\n",
        "\n",
        "df['text'] = df['text'].map(remove_something)\n",
        "\n",
        "def remove_something(text:str):\n",
        "  return text.replace(',',' ')\n",
        "\n",
        "df['text'] = df['text'].map(remove_something)"
      ],
      "metadata": {
        "id": "9I3hlU2nyue1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGWbl8edlz_5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "\n",
        "checkpoint = \"monologg/koelectra-base-v3-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['labels'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "J3OrqrY3or6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "n2UNjp014ty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacing = Spacing()\n",
        "\n",
        "def text_space(text:str):\n",
        "  return spacing(text)"
      ],
      "metadata": {
        "id": "XdEN1rkt2VEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 띄어쓰기 전처리"
      ],
      "metadata": {
        "id": "pU0MphF04vFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,11):\n",
        "  my_file = Path(\"/content/drive/MyDrive/220407_inference/train\"+str(i))\n",
        "  if my_file.is_file():\n",
        "    continue\n",
        "  start = (i-1)*100000\n",
        "  end = i*100000\n",
        "  temp = df[start:end]\n",
        "  temp['text'] = temp['text'].map(text_space)\n",
        "  temp.to_csv('/content/drive/MyDrive/220407_inference/train'+str(i)+'.csv', index=False)"
      ],
      "metadata": {
        "id": "BTmFdqD_1VKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/220407_inference/train1.csv')\n",
        "\n",
        "for i in range(2, 11):\n",
        "  temp = pd.read_csv('/content/drive/MyDrive/220407_inference/train'+str(i)+'.csv')\n",
        "  df = pd.concat([df,temp], axis=0)"
      ],
      "metadata": {
        "id": "3Oq77sqN3DLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "6WMbCFpJ5i0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.fillna('')\n",
        "test['text'] = test['text_obj'].astype(str) + \" \" + test['text_mthd'].astype(str) + \" \" + test['text_deal'].astype(str)\n",
        "test['target'] = test['digit_1'].astype(str) + \" \" + test['digit_2'].astype(str) + \" \" + test['digit_3'].astype(str)\n",
        "test = test[['text', 'target']]\n",
        "\n",
        "def remove_something(text:str):\n",
        "  return text.replace('&',' ')\n",
        "\n",
        "test['text'] = test['text'].map(remove_something)\n",
        "\n",
        "def remove_something(text:str):\n",
        "  return text.replace('.',' ')\n",
        "\n",
        "test['text'] = test['text'].map(remove_something)\n",
        "\n",
        "def remove_something(text:str):\n",
        "  return text.replace(',',' ')\n",
        "\n",
        "test['text'] = test['text'].map(remove_something)\n",
        "\n",
        "test_original = test"
      ],
      "metadata": {
        "id": "uAYuiiHv6_2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습\n",
        "\n",
        "* Model\n",
        "  + Roberta-large\n",
        "\n",
        "* Optimizer\n",
        "  + AdamW\n",
        "\n",
        "* Learning rate scheduler\n",
        "  + Cosine annealing with warmup"
      ],
      "metadata": {
        "id": "uep3CPkqCWWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_function():\n",
        "  accelerator = Accelerator()\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, \n",
        "                    batch_size=32)\n",
        "  # eval_dataloader = torch.utils.data.DataLoader(\n",
        "  #                   train_dataset,\n",
        "  #                   batch_size=32, sampler=test_subsampler)\n",
        "\n",
        "  config = AutoConfig.from_pretrained(checkpoint)\n",
        "  config.num_labels = len(train['target'].unique())\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "  model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
        "\n",
        "  num_epochs = 10\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=1,\n",
        "      num_training_steps=num_training_steps,\n",
        "  )\n",
        "  for epoch in range(num_epochs):\n",
        "      train_acc = 0.0\n",
        "      test_acc = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for batch_id, batch in enumerate(train_dataloader):\n",
        "          outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "          loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "          accelerator.backward(loss)\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "          train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "      print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "      gc.collect()\n",
        "  accelerator.wait_for_everyone()\n",
        "  unwrapped_model = accelerator.unwrap_model(model)\n",
        "  unwrapped_model.save_pretrained('/content/drive/MyDrive/220407_inference/model'+str(i), save_function=accelerator.save)"
      ],
      "metadata": {
        "id": "miLIkS4ZQnxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## digit 별 학습"
      ],
      "metadata": {
        "id": "zak9QoZ24ynE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,4):\n",
        "  my_file = Path(\"/content/drive/MyDrive/220407_inference/model\"+str(i))\n",
        "  if my_file.is_dir():\n",
        "    continue\n",
        "\n",
        "  train = df[['text']]\n",
        "  target = 'digit_'+str(i)\n",
        "  train['target'] = df[target]\n",
        "\n",
        "  my_dict = {}\n",
        "  for i in range(len(train['target'].unique())):\n",
        "    my_dict[train['target'].unique()[i]] = i\n",
        "\n",
        "  def target_to_num(target:str):\n",
        "    return my_dict[target]\n",
        "\n",
        "  train['target'] = train['target'].map(target_to_num)\n",
        "  my_dict['  '] = len(my_dict)\n",
        "\n",
        "  print('--------------------------------')\n",
        "\n",
        "  tokenized_train = tokenizer(\n",
        "    list(train['text']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=64, # Max_Length = 64\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        "    )\n",
        "    \n",
        "  train_label = train['target']\n",
        "\n",
        "  train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "\n",
        "  notebook_launcher(training_function)\n",
        "\n",
        "  test = test_original\n",
        "  \n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  test_label = test['target'].map(target_to_num)\n",
        "\n",
        "  tokenized_test = tokenizer(\n",
        "      list(test['text']),\n",
        "      return_tensors=\"pt\",\n",
        "      max_length=64,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True\n",
        "  )\n",
        "\n",
        "  test_dataset = BERTDataset(tokenized_test, test_label)\n",
        "\n",
        "  dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "  config = AutoConfig.from_pretrained(checkpoint)\n",
        "  config.num_labels = len(train['target'].unique())\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/220407_inference/model'+str(i), config=config)\n",
        "  model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "  accelerator = Accelerator()\n",
        "  model = accelerator.unwrap_model(model)\n",
        "\n",
        "  output_pred = []\n",
        "  output_prob = []\n",
        "\n",
        "  model, dataloader= accelerator.prepare(model, dataloader)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for i, data in enumerate(tqdm(dataloader)):\n",
        "      with torch.no_grad():\n",
        "          outputs = model(\n",
        "              input_ids=data['input_ids'],\n",
        "              attention_mask=data['attention_mask']\n",
        "          )\n",
        "      logits = outputs[0]\n",
        "      prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      result = np.argmax(logits, axis=-1)\n",
        "      output_pred.append(result)\n",
        "      output_prob.append(prob)\n",
        "      \n",
        "  pred_answer = np.concatenate(output_pred).tolist()\n",
        "\n",
        "  def num_to_label(label):\n",
        "      label_dict = {v: k for k, v in my_dict.items()}\n",
        "      str_label = []\n",
        "\n",
        "      for i, v in enumerate(label):\n",
        "          str_label.append([i,label_dict[v]])\n",
        "      \n",
        "      return str_label\n",
        "\n",
        "  answer = num_to_label(pred_answer)\n",
        "\n",
        "  df_label = pd.DataFrame(answer, columns=['index', 'label'])\n",
        "\n",
        "  df_label.to_csv('/content/drive/MyDrive/220407_inference/pred_label'+str(i)+'.csv', index=False)"
      ],
      "metadata": {
        "id": "g9EYCLPy4H_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "id": "ybhmYNaOfCX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "m94JxCJRuijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label = pd.read_csv('/content/drive/MyDrive/220406_inference/pred_label.csv')"
      ],
      "metadata": {
        "id": "npXR96pIBZl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label"
      ],
      "metadata": {
        "id": "Qke27UBHwn2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_label(text:str):\n",
        "  return text.split(' ')[0]\n",
        "\n",
        "submission['digit_1'] = pred_label['label'].map(split_label)\n",
        "\n",
        "def split_label(text:str):\n",
        "  return text.split(' ')[1]\n",
        "\n",
        "submission['digit_2'] = pred_label['label'].map(split_label)\n",
        "\n",
        "def split_label(text:str):\n",
        "  return text.split(' ')[2]\n",
        "\n",
        "submission['digit_3'] = pred_label['label'].map(split_label)"
      ],
      "metadata": {
        "id": "wrKxzOtwv4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "id": "K4c6Zg0z4DXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "Jst5B_vZpBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/220406_inference/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "P99C7cKApD6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "220315_sm_SANUP_TPU.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}